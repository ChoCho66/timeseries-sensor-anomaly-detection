{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 長度都是 20480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ims_bearing/Readme Document for IMS Bearing Data.pdf\n",
      "ims_bearing/archive.zip\n",
      "ims_bearing/2nd_test/2nd_test/2004.02.17.18.52.39\n",
      "ims_bearing/2nd_test/2nd_test/2004.02.13.01.12.39\n",
      "ims_bearing/2nd_test/2nd_test/2004.02.19.04.22.39\n",
      "ims_bearing/1st_test/1st_test/2003.11.01.11.41.44\n",
      "ims_bearing/1st_test/1st_test/2003.11.01.02.21.44\n",
      "ims_bearing/1st_test/1st_test/2003.11.18.14.32.30\n",
      "ims_bearing/3rd_test/4th_test/txt/2004.03.25.16.51.57\n",
      "ims_bearing/3rd_test/4th_test/txt/2004.04.17.02.22.55\n",
      "ims_bearing/3rd_test/4th_test/txt/2004.04.06.12.21.57\n"
     ]
    }
   ],
   "source": [
    "# 列出 ims_bearing 裡頭所有子目錄的 csv 檔案\n",
    "import os\n",
    "import pandas as pd\n",
    "path = 'ims_bearing'\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files[:3]:\n",
    "        print(os.path.join(root, file))\n",
    "        try:\n",
    "            df = pd.read_csv(file, sep='\\t', header=None)\n",
    "            print(len(df))\n",
    "            print(df.head())\n",
    "            print(df.tail())\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2156\n",
      "            Unnamed: 0    b1_ch1    b1_ch2    b2_ch3    b2_ch4    b3_ch5  \\\n",
      "0  2003-10-22 12:06:24  0.104144  0.100250  0.107151  0.102002  0.106148   \n",
      "1  2003-10-22 12:09:13  0.103652  0.099853  0.108190  0.102922  0.106660   \n",
      "2  2003-10-22 12:14:13  0.105036  0.101540  0.108548  0.104041  0.108741   \n",
      "3  2003-10-22 12:19:13  0.104898  0.101570  0.108150  0.103379  0.108073   \n",
      "4  2003-10-22 12:24:13  0.104776  0.102188  0.107943  0.102626  0.108453   \n",
      "\n",
      "     b3_ch6    b4_ch7    b4_ch8  \n",
      "0  0.108152  0.094804  0.099515  \n",
      "1  0.108453  0.095070  0.093585  \n",
      "2  0.109881  0.096157  0.098300  \n",
      "3  0.110006  0.096813  0.098600  \n",
      "4  0.109349  0.096359  0.098468  \n",
      "               Unnamed: 0    b1_ch1    b1_ch2    b2_ch3    b2_ch4    b3_ch5  \\\n",
      "2151  2003-11-25 16:07:32  0.138781  0.130003  0.151368  0.138563  0.209854   \n",
      "2152  2003-11-25 23:13:21  0.142146  0.131044  0.162769  0.149215  0.339316   \n",
      "2153  2003-11-25 23:19:56  0.142098  0.131344  0.167268  0.154293  0.357835   \n",
      "2154  2003-11-25 23:29:56  0.141899  0.132984  0.158428  0.153233  0.331161   \n",
      "2155  2003-11-25 23:39:56  0.142011  0.131697  0.170624  0.162340  0.438412   \n",
      "\n",
      "        b3_ch6    b4_ch7    b4_ch8  \n",
      "2151  0.217345  0.165272  0.154722  \n",
      "2152  0.330129  0.175639  0.163235  \n",
      "2153  0.351630  0.184360  0.165717  \n",
      "2154  0.335419  0.174020  0.161536  \n",
      "2155  0.364026  0.181261  0.168048  \n"
     ]
    }
   ],
   "source": [
    "# open \"csv_data/avg_concat_dataset_1.csv\" (python)\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"csv_data/avg_concat_dataset_1.csv\")\n",
    "print(len(df))\n",
    "print(df.head())\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      date        b1_ch1        b1_ch2        b2_ch3  \\\n",
      "0      2003-10-22 12:06:24  5.119589e+06  5.108868e+06  5.111375e+06   \n",
      "1      2003-10-22 12:06:24  7.383400e+04  7.393105e+04  7.380612e+04   \n",
      "2      2003-10-22 12:06:24  7.125052e+04  7.120187e+04  7.101303e+04   \n",
      "3      2003-10-22 12:06:24  6.510619e+04  6.556233e+04  6.499503e+04   \n",
      "4      2003-10-22 12:06:24  6.271328e+04  6.206245e+04  6.190362e+04   \n",
      "...                    ...           ...           ...           ...   \n",
      "49995  2003-10-22 12:14:13  3.818222e+02  3.500611e+02  3.252055e+02   \n",
      "49996  2003-10-22 12:14:13  8.823327e+02  4.311399e+02  1.896097e+02   \n",
      "49997  2003-10-22 12:14:13  2.253729e+02  5.971241e+02  3.620366e+02   \n",
      "49998  2003-10-22 12:14:13  2.101041e+02  6.188112e+02  6.091051e+02   \n",
      "49999  2003-10-22 12:14:13  5.189123e+02  4.861485e+02  2.371682e+02   \n",
      "\n",
      "             b2_ch4        b3_ch5        b3_ch6        b4_ch7        b4_ch8  \n",
      "0      5.106550e+06  4.978927e+06  4.984062e+06  4.982843e+06  4.977161e+06  \n",
      "1      7.386224e+04  7.493522e+04  7.081826e+04  7.159109e+04  6.415226e+04  \n",
      "2      7.158672e+04  7.350542e+04  6.922753e+04  7.277226e+04  6.603898e+04  \n",
      "3      6.495063e+04  6.795313e+04  6.428079e+04  6.317480e+04  5.856575e+04  \n",
      "4      6.224671e+04  6.404001e+04  6.095201e+04  6.101940e+04  6.014753e+04  \n",
      "...             ...           ...           ...           ...           ...  \n",
      "49995  3.037995e+02  5.510142e+02  2.521116e+02  4.779070e+02  4.163124e+02  \n",
      "49996  3.348276e+02  3.074335e+02  1.811918e+02  3.781961e+02  6.299172e+02  \n",
      "49997  3.543184e+02  3.024321e+02  2.831417e+02  2.551215e+01  3.340607e+02  \n",
      "49998  2.474949e+02  9.574545e+02  9.167681e+02  3.834395e+02  6.174723e+02  \n",
      "49999  5.509830e+02  3.589416e+02  1.731688e+02  2.069931e+02  6.834912e+02  \n",
      "\n",
      "[50000 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 只讀取前 10 項目\n",
    "df = pd.read_csv('csv_data/fft_dataset_1.csv', nrows=50000)\n",
    "# 顯示結果\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>b1_ch1</th>\n",
       "      <th>b1_ch2</th>\n",
       "      <th>b2_ch3</th>\n",
       "      <th>b2_ch4</th>\n",
       "      <th>b3_ch5</th>\n",
       "      <th>b3_ch6</th>\n",
       "      <th>b4_ch7</th>\n",
       "      <th>b4_ch8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2003-10-22 12:06:24</td>\n",
       "      <td>5.119589e+06</td>\n",
       "      <td>5.108868e+06</td>\n",
       "      <td>5.111375e+06</td>\n",
       "      <td>5.106550e+06</td>\n",
       "      <td>4.978927e+06</td>\n",
       "      <td>4.984062e+06</td>\n",
       "      <td>4.982843e+06</td>\n",
       "      <td>4.977161e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2003-10-22 12:06:24</td>\n",
       "      <td>7.383400e+04</td>\n",
       "      <td>7.393105e+04</td>\n",
       "      <td>7.380612e+04</td>\n",
       "      <td>7.386224e+04</td>\n",
       "      <td>7.493522e+04</td>\n",
       "      <td>7.081826e+04</td>\n",
       "      <td>7.159109e+04</td>\n",
       "      <td>6.415226e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2003-10-22 12:06:24</td>\n",
       "      <td>7.125052e+04</td>\n",
       "      <td>7.120187e+04</td>\n",
       "      <td>7.101303e+04</td>\n",
       "      <td>7.158672e+04</td>\n",
       "      <td>7.350542e+04</td>\n",
       "      <td>6.922753e+04</td>\n",
       "      <td>7.277226e+04</td>\n",
       "      <td>6.603898e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2003-10-22 12:06:24</td>\n",
       "      <td>6.510619e+04</td>\n",
       "      <td>6.556233e+04</td>\n",
       "      <td>6.499503e+04</td>\n",
       "      <td>6.495063e+04</td>\n",
       "      <td>6.795313e+04</td>\n",
       "      <td>6.428079e+04</td>\n",
       "      <td>6.317480e+04</td>\n",
       "      <td>5.856575e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2003-10-22 12:06:24</td>\n",
       "      <td>6.271328e+04</td>\n",
       "      <td>6.206245e+04</td>\n",
       "      <td>6.190362e+04</td>\n",
       "      <td>6.224671e+04</td>\n",
       "      <td>6.404001e+04</td>\n",
       "      <td>6.095201e+04</td>\n",
       "      <td>6.101940e+04</td>\n",
       "      <td>6.014753e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20475</th>\n",
       "      <td>2003-10-22 12:06:24</td>\n",
       "      <td>2.368670e+02</td>\n",
       "      <td>5.272867e+02</td>\n",
       "      <td>7.752849e+01</td>\n",
       "      <td>3.301957e+02</td>\n",
       "      <td>4.150420e+02</td>\n",
       "      <td>4.809046e+02</td>\n",
       "      <td>4.720241e+02</td>\n",
       "      <td>3.922110e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20476</th>\n",
       "      <td>2003-10-22 12:06:24</td>\n",
       "      <td>3.234646e+02</td>\n",
       "      <td>3.780511e+02</td>\n",
       "      <td>1.307320e+02</td>\n",
       "      <td>4.765706e+02</td>\n",
       "      <td>5.768678e+02</td>\n",
       "      <td>4.905335e+02</td>\n",
       "      <td>2.774201e+02</td>\n",
       "      <td>5.043436e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20477</th>\n",
       "      <td>2003-10-22 12:06:24</td>\n",
       "      <td>1.754604e+02</td>\n",
       "      <td>8.622013e+02</td>\n",
       "      <td>1.324796e+02</td>\n",
       "      <td>4.032614e+02</td>\n",
       "      <td>8.390634e+02</td>\n",
       "      <td>6.591582e+02</td>\n",
       "      <td>4.620751e+02</td>\n",
       "      <td>4.488147e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20478</th>\n",
       "      <td>2003-10-22 12:06:24</td>\n",
       "      <td>5.117090e+02</td>\n",
       "      <td>3.521754e+02</td>\n",
       "      <td>2.616373e+02</td>\n",
       "      <td>2.507592e+02</td>\n",
       "      <td>2.953248e+02</td>\n",
       "      <td>4.943595e+02</td>\n",
       "      <td>1.503133e+02</td>\n",
       "      <td>5.448555e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20479</th>\n",
       "      <td>2003-10-22 12:06:24</td>\n",
       "      <td>6.461889e+02</td>\n",
       "      <td>4.829047e+01</td>\n",
       "      <td>4.422110e+02</td>\n",
       "      <td>2.583418e+02</td>\n",
       "      <td>6.169873e+02</td>\n",
       "      <td>7.692588e+02</td>\n",
       "      <td>4.295356e+02</td>\n",
       "      <td>2.919584e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20480 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      date        b1_ch1        b1_ch2        b2_ch3  \\\n",
       "0      2003-10-22 12:06:24  5.119589e+06  5.108868e+06  5.111375e+06   \n",
       "1      2003-10-22 12:06:24  7.383400e+04  7.393105e+04  7.380612e+04   \n",
       "2      2003-10-22 12:06:24  7.125052e+04  7.120187e+04  7.101303e+04   \n",
       "3      2003-10-22 12:06:24  6.510619e+04  6.556233e+04  6.499503e+04   \n",
       "4      2003-10-22 12:06:24  6.271328e+04  6.206245e+04  6.190362e+04   \n",
       "...                    ...           ...           ...           ...   \n",
       "20475  2003-10-22 12:06:24  2.368670e+02  5.272867e+02  7.752849e+01   \n",
       "20476  2003-10-22 12:06:24  3.234646e+02  3.780511e+02  1.307320e+02   \n",
       "20477  2003-10-22 12:06:24  1.754604e+02  8.622013e+02  1.324796e+02   \n",
       "20478  2003-10-22 12:06:24  5.117090e+02  3.521754e+02  2.616373e+02   \n",
       "20479  2003-10-22 12:06:24  6.461889e+02  4.829047e+01  4.422110e+02   \n",
       "\n",
       "             b2_ch4        b3_ch5        b3_ch6        b4_ch7        b4_ch8  \n",
       "0      5.106550e+06  4.978927e+06  4.984062e+06  4.982843e+06  4.977161e+06  \n",
       "1      7.386224e+04  7.493522e+04  7.081826e+04  7.159109e+04  6.415226e+04  \n",
       "2      7.158672e+04  7.350542e+04  6.922753e+04  7.277226e+04  6.603898e+04  \n",
       "3      6.495063e+04  6.795313e+04  6.428079e+04  6.317480e+04  5.856575e+04  \n",
       "4      6.224671e+04  6.404001e+04  6.095201e+04  6.101940e+04  6.014753e+04  \n",
       "...             ...           ...           ...           ...           ...  \n",
       "20475  3.301957e+02  4.150420e+02  4.809046e+02  4.720241e+02  3.922110e+02  \n",
       "20476  4.765706e+02  5.768678e+02  4.905335e+02  2.774201e+02  5.043436e+02  \n",
       "20477  4.032614e+02  8.390634e+02  6.591582e+02  4.620751e+02  4.488147e+02  \n",
       "20478  2.507592e+02  2.953248e+02  4.943595e+02  1.503133e+02  5.448555e+02  \n",
       "20479  2.583418e+02  6.169873e+02  7.692588e+02  4.295356e+02  2.919584e+02  \n",
       "\n",
       "[20480 rows x 9 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = df[df['date'] == \"2003-10-22 12:06:24\"]\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        5.119589e+06\n",
       "1        7.383400e+04\n",
       "2        7.125052e+04\n",
       "3        6.510619e+04\n",
       "4        6.271328e+04\n",
       "             ...     \n",
       "20475    2.368670e+02\n",
       "20476    3.234646e+02\n",
       "20477    1.754604e+02\n",
       "20478    5.117090e+02\n",
       "20479    6.461889e+02\n",
       "Name: b1_ch1, Length: 20480, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "__ = _['b1_ch1']\n",
    "__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1.000000\n",
       "1        0.014422\n",
       "2        0.013917\n",
       "3        0.012717\n",
       "4        0.012250\n",
       "           ...   \n",
       "20475    0.000046\n",
       "20476    0.000063\n",
       "20477    0.000034\n",
       "20478    0.000100\n",
       "20479    0.000126\n",
       "Name: b1_ch1, Length: 20480, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = df[df['date'] == \"2003-10-22 12:06:24\"] \n",
    "(_['b1_ch1']/_['b1_ch1'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([15.03276443,  8.62930023, 11.82992191,  2.98899442,  6.09105752]),\n",
       " array([ 6.09105752,  2.98899442, 11.82992191,  8.62930023, 15.03276443]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.fft import fft\n",
    "from scipy.signal import detrend\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"ims_bearing/1st_test/1st_test/2003.10.22.12.06.24\", sep='\\t', header=None)\n",
    "____ = abs(fft(df1[0].values)[1:])\n",
    "____[:5], ____[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44154881"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rows = sum(1 for line in open('csv_data/fft_dataset_1.csv'))\n",
    "total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "44154880/20480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6324\n"
     ]
    }
   ],
   "source": [
    "# 找出這資料夾裡面有多少檔案 ims_bearing/3rd_test\n",
    "import os\n",
    "\n",
    "path = 'ims_bearing/3rd_test/4th_test/txt'\n",
    "files = os.listdir(path)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.33226763e-15, -8.88178420e-16,  0.00000000e+00])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.signal import detrend\n",
    "detrend([3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 dataset_path1 ./ims_bearing/1st_test/1st_test\n",
      "1 dataset_path2 ./ims_bearing/2nd_test/2nd_test\n",
      "2 dataset_path3 ./ims_bearing/3rd_test/4th_test/txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from natsort import natsorted\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import detrend\n",
    "\n",
    "\n",
    "def dir_list(path=None):\n",
    "    \"\"\"\n",
    "    :param path: path to the dataset\n",
    "    :return: an order directory list\n",
    "    \"\"\"\n",
    "    list_dir = natsorted(os.listdir(path))\n",
    "    return list_dir\n",
    "\n",
    "\n",
    "def concat_raw_data(path=None, csv_path=None, dataset=None, fourier_tr=True,\n",
    "                    detrends=True):\n",
    "    \"\"\"\n",
    "    :param detrends: boolean to detrend the signal\n",
    "    :param fourier_tr: boolean tranfor singal with furier transform\n",
    "    :param path: a path to the dataset file\n",
    "    :param dataset: number of the dataset to be processes from the three dataset available\n",
    "    :param csv_path: path save the contenated files into a csv file\n",
    "    :return: data dataset with average and std from each file at each time step\n",
    "    \"\"\"\n",
    "\n",
    "    list_dir = dir_list(path)\n",
    "\n",
    "    col_dual = list()\n",
    "    for b in range(0, 4):\n",
    "        b1 = f'b{b + 1}_ch{b * 2 + 1}'\n",
    "        b2 = f'b{b + 1}_ch{b * 2 + 2}'\n",
    "        col_dual.extend([b1, b2])\n",
    "\n",
    "    col_names = [f'b{i + 1}_ch{i + 1}' for i in range(0, 4)]\n",
    "\n",
    "    dataset_dict = {}\n",
    "\n",
    "    for i, f in enumerate(list_dir):\n",
    "        temp_df = pd.read_csv(os.path.join(path, f), sep='\\t', header=None)\n",
    "        if len(temp_df.columns) == 8:\n",
    "            temp_df.columns = col_dual\n",
    "        else:\n",
    "            temp_df.columns = col_names\n",
    "        temp_df.insert(0, 'date', len(temp_df) * [f])\n",
    "        dataset_dict[f] = temp_df\n",
    "\n",
    "    df = pd.concat(list(dataset_dict.values()), ignore_index=True)\n",
    "    df.set_index('date', inplace=True)\n",
    "    df.index = pd.to_datetime(df.index, format='%Y.%m.%d.%H.%M.%S')\n",
    "\n",
    "    os.makedirs(csv_path, exist_ok=True)\n",
    "\n",
    "    if fourier_tr:\n",
    "        return fourier_transforms(df, path=csv_path, dataset=dataset, detrends=detrends)\n",
    "\n",
    "    else:\n",
    "        fname = os.path.join(csv_path, f'concat_dataset_{dataset}.csv')\n",
    "\n",
    "        return df.to_csv(fname)\n",
    "\n",
    "\n",
    "def fourier_transforms(data_frame, path=None, dataset=1, detrends=True):\n",
    "    \"\"\"\n",
    "    :param data_frame: datafram with the concatenated raw data\n",
    "    :param path: a path to store the csv file\n",
    "    :param dataset: number of dataste processed\n",
    "    :param detrends: a boolean to detrend before applyin fourier transformations\n",
    "    :return: save dataframe as csv\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    fname = os.path.join(path, f'fft_dataset_{dataset}.csv')\n",
    "    df_fft = data_frame.copy()\n",
    "\n",
    "    for col in df_fft.columns:\n",
    "        if detrends:\n",
    "            fft_col = fft(detrend(df_fft[col].values))\n",
    "        else:\n",
    "            fft_col = fft(df_fft[col].values)\n",
    "\n",
    "        df_fft[col] = np.abs(fft_col)\n",
    "\n",
    "    return df_fft.to_csv(fname)\n",
    "\n",
    "\n",
    "def average_signal_dataset(path=None, dataset=1, csv_path=None):\n",
    "    \"\"\"\n",
    "    :param path:  str a path to the dataset file\n",
    "    :param dataset:  int the number of the daset to be process\n",
    "    :param csv_path:  str a path to save the datframes to csv file\n",
    "    :return: data dataset with average and std from each file at each time step\n",
    "    \"\"\"\n",
    "    list_dir = dir_list(path)\n",
    "\n",
    "    if dataset == 1:\n",
    "        col_names = list()\n",
    "        for b in range(0, 4):\n",
    "            b1 = f'b{b + 1}_ch{b * 2 + 1}'\n",
    "            b2 = f'b{b + 1}_ch{b * 2 + 2}'\n",
    "            col_names.extend([b1, b2])\n",
    "    else:\n",
    "        col_names = [f'b{i + 1}_ch{i + 1}' for i in range(0, 4)]\n",
    "\n",
    "    dataset_dict = {}\n",
    "\n",
    "    for file in list_dir:\n",
    "        temp_df = pd.read_csv(os.path.join(path, file), sep='\\t', header=None)\n",
    "        # mean_std_values = np.append(temp_df.abs().mean().values, temp_df.abs().std().values)\n",
    "        mean = temp_df.abs().mean().values\n",
    "        dataset_dict[file] = mean\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dataset_dict, orient='index', columns=col_names)\n",
    "    df.index = pd.to_datetime(df.index, format='%Y.%m.%d.%H.%M.%S')\n",
    "    os.makedirs(csv_path, exist_ok=True)\n",
    "\n",
    "    fname = os.path.join(csv_path, f'avg_concat_dataset_{dataset}.csv')\n",
    "\n",
    "    return df.to_csv(fname)\n",
    "\n",
    "\n",
    "def generate_datasets(datasets_dict=None, csv_path=None):\n",
    "    for i, (k, v) in enumerate(datasets_dict.items()):\n",
    "        dataset_num = i + 1\n",
    "        print(i,k,v)\n",
    "        # average_signal_dataset(v, dataset=dataset_num, csv_path=csv_path)\n",
    "        # concat_raw_data(v, csv_path, dataset=dataset_num, fourier_tr=True, detrends=False)\n",
    "\n",
    "\n",
    "datasets = {'dataset_path1': './ims_bearing/1st_test/1st_test',\n",
    "            'dataset_path2': './ims_bearing/2nd_test/2nd_test',\n",
    "            'dataset_path3': './ims_bearing/3rd_test/4th_test/txt'}\n",
    "\n",
    "csv_dir = os.path.join(os.getcwd(), 'csv_data')\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "generate_datasets(datasets, csv_path=csv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kycho/timeseries-sensor-anomaly-detection/.conda/lib/python3.11/site-packages/gdown/parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=1IYO-5MkHCZp-IYQ-44jPxR1Ux4t_NABR\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/file/d/1IYO-5MkHCZp-IYQ-44jPxR1Ux4t_NABR/view?usp=sharing\n",
      "To: /home/kycho/timeseries-sensor-anomaly-detection/archive.zip\n",
      "92.9kB [00:00, 6.71MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "檔案已下載到: archive.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import gdown\n",
    "\n",
    "# # Google Drive 檔案的分享連結\n",
    "# url = \"https://drive.google.com/file/d/1IYO-5MkHCZp-IYQ-44jPxR1Ux4t_NABR/view?usp=sharing\"\n",
    "\n",
    "# # 目標檔案的存放位置\n",
    "# output = \"archive.zip\"  # 替換為你的檔案名稱和格式，例如 example.txt, model.pth 等\n",
    "\n",
    "# # 使用 gdown 下載\n",
    "# gdown.download(url, output, quiet=False)\n",
    "\n",
    "# print(f\"檔案已下載到: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
